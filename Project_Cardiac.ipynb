{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ECG.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPW-SywfJ-0r"
      },
      "source": [
        "\n",
        "\n",
        "**Mount The GOOGLE DRIVE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEoC5KFDGVVM"
      },
      "source": [
        "#Mount Google Drive \n",
        "from google.colab import drive \n",
        "drive.mount('/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XICco3rOUe8x"
      },
      "source": [
        "#navigate to Project-folder (Folder Name = ECG_Project)\n",
        "%cd /drive/'My Drive'/ECG_Project/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIlwkSAqIOh-"
      },
      "source": [
        "#installing Required packges \n",
        "!apt-get install -qq protobuf-compiler python-pil python-lxml python-tk\n",
        "!pip install -qq Cython contextlib2 pillow lxml matplotlib pycocotools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaWWkJNoIUc5"
      },
      "source": [
        "#installing Required Versions of Tensor Flow and Numpy\n",
        "!pip install tensorflow==1.15.0\n",
        "!pip install numpy==1.17.4\n",
        "!pip install tf_slim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8OKVsyvKiDV"
      },
      "source": [
        "**Restart runtime (Before executing the next code)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aizCbTRxIUff"
      },
      "source": [
        "#check tensor-flow Version\n",
        "import tensorflow as tf\n",
        "%tensorflow_version 1.x\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ma7-ChlIacG"
      },
      "source": [
        "#import Required Packages\n",
        "from __future__ import division, print_function, absolute_import\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import pycocotools\n",
        "import re\n",
        "import os\n",
        "import io\n",
        "import glob\n",
        "import shutil\n",
        "import urllib.request\n",
        "import tarfile\n",
        "import xml.etree.ElementTree as ET\n",
        "import tensorflow.compat.v1 as tf\n",
        "import cv2 \n",
        "from PIL import Image\n",
        "from collections import namedtuple, OrderedDict\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_DViFqRIaeX"
      },
      "source": [
        "#Go to Project Folder  \n",
        "%cd /drive/My Drive/ECG_Project/data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68SXz7vYIa6W"
      },
      "source": [
        "#Navigate to Data-Folde\n",
        "%cd /drive/My Drive/ECG_Project/data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYzHXNYzIabW"
      },
      "source": [
        "#Go to Data Folder\n",
        "%cd /drive/My Drive/ECG_Project/data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYIfrlZFIuwu"
      },
      "source": [
        "#Convert XML to CSV and save into data-folder\n",
        "def xml_to_csv(path):\n",
        "  classes_names = []\n",
        "  xml_list = []\n",
        "\n",
        "  for xml_file in glob.glob(path + '/*.xml'):\n",
        "    tree = ET.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "    for member in root.findall('object'):\n",
        "      classes_names.append(member[0].text)\n",
        "      value = (root.find('filename').text ,\n",
        "               int(root.find('size')[0].text),\n",
        "               int(root.find('size')[1].text),\n",
        "               member[0].text,\n",
        "               int(member[4][0].text),\n",
        "               int(member[4][1].text),\n",
        "               int(member[4][2].text),\n",
        "               int(member[4][3].text))\n",
        "      xml_list.append(value)\n",
        "  column_name = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\n",
        "  xml_df = pd.DataFrame(xml_list, columns=column_name) \n",
        "  classes_names = list(set(classes_names))\n",
        "  classes_names.sort()\n",
        "  return xml_df, classes_names\n",
        "\n",
        "for label_path in ['train_labels', 'test_labels']:\n",
        "  image_path = os.path.join(os.getcwd(), label_path)\n",
        "  xml_df, classes = xml_to_csv(label_path)\n",
        "  xml_df.to_csv(f'{label_path}.csv', index=None)\n",
        "  print(f'Successfully converted {label_path} xml to csv.')\n",
        "\n",
        "label_map_path = os.path.join(\"label_map.pbtxt\")\n",
        "pbtxt_content = \"\"\n",
        "\n",
        "for i, class_name in enumerate(classes):\n",
        "    pbtxt_content = (\n",
        "        pbtxt_content\n",
        "        + \"item {{\\n    id: {0}\\n    name: '{1}'\\n}}\\n\\n\".format(i + 1, class_name)\n",
        "    )\n",
        "pbtxt_content = pbtxt_content.strip()\n",
        "with open(label_map_path, \"w\") as f:\n",
        "    f.write(pbtxt_content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVbRIOGdI0NR"
      },
      "source": [
        "%cd /drive/My Drive/ECG_Project/data/models/research/\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IdulUYII6V3"
      },
      "source": [
        "# compile the proto buffers\n",
        "!protoc object_detection/protos/*.proto --python_out=."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utmIhqR6I85J"
      },
      "source": [
        "# exports PYTHONPATH environment var with research and slim paths\n",
        "os.environ['PYTHONPATH'] += ':./:./slim/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AjP1kb3I-GO"
      },
      "source": [
        "# testing the model builder\n",
        "!python3 object_detection/builders/model_builder_test.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOkQrahfJJGf"
      },
      "source": [
        "#CSV to TF-Record (NONO)\n",
        "from object_detection.utils import dataset_util\n",
        "\n",
        "\n",
        "#change this to the base directory where your data/ is \n",
        "data_base_url = '/drive/MyDrive/ECG_Project/data/'\n",
        "\n",
        "#location of images\n",
        "image_dir = data_base_url +'images/'\n",
        "\n",
        "def class_text_to_int(row_label):\n",
        "\t\tif row_label == 'I-HB':\n",
        "\t\t\treturn 1\n",
        "\t\telif row_label=='I-MI':\n",
        "\t\t\treturn 2\t\n",
        "\t\telif row_label=='I-Normal':\n",
        "\t\t\treturn 3\n",
        "\t\telif row_label=='I-PMI':\n",
        "\t\t\treturn 4\t\n",
        "\t\telif row_label=='II-HB':\n",
        "\t\t\treturn 5\t\n",
        "\t\telif row_label=='II-MI':\n",
        "\t\t\treturn 6\t\n",
        "\t\telif row_label=='II-Normal':\n",
        "\t\t\treturn 7\t\n",
        "\t\telif row_label=='II-PMI':\n",
        "\t\t\treturn 8\t\n",
        "\t\telif row_label=='III-HB':\n",
        "\t\t\treturn 9\t\n",
        "\t\telif row_label=='III-MI':\n",
        "\t\t\treturn 10\t\n",
        "\t\telif row_label=='III-Normal':\n",
        "\t\t\treturn 11\t\n",
        "\t\telif row_label=='III-PMI':\n",
        "\t\t\treturn 12\t\n",
        "\t\telif row_label=='V1-HB':\n",
        "\t\t\treturn 13\t\n",
        "\t\telif row_label=='V1-MI':\n",
        "\t\t\treturn 14\t\n",
        "\t\telif row_label=='V1-Normal':\n",
        "\t\t\treturn 15\t\n",
        "\t\telif row_label=='V1-PMI':\n",
        "\t\t\treturn 16\t\n",
        "\t\telif row_label=='V2-HB':\n",
        "\t\t\treturn 17\t\n",
        "\t\telif row_label=='V2-MI':\n",
        "\t\t\treturn 18\t\n",
        "\t\telif row_label=='V2-Normal':\n",
        "\t\t\treturn 19\t\n",
        "\t\telif row_label=='V2-PMI':\n",
        "\t\t\treturn 20\t\n",
        "\t\telif row_label=='V3-HB':\n",
        "\t\t\treturn 21\t\n",
        "\t\telif row_label=='V3-MI':\n",
        "\t\t\treturn 22\t\n",
        "\t\telif row_label=='V3-Normal':\n",
        "\t\t\treturn 23\t\n",
        "\t\telif row_label=='V3-PMI':\n",
        "\t\t\treturn 24\t\n",
        "\t\telif row_label=='V4-HB':\n",
        "\t\t\treturn 25\t\n",
        "\t\telif row_label=='V4-MI':\n",
        "\t\t\treturn 26\t\n",
        "\t\telif row_label=='V4-PMI':\n",
        "\t\t\treturn 27\t\n",
        "\t\telif row_label=='V5-HB':\n",
        "\t\t\treturn 28\t\n",
        "\t\telif row_label=='V5-MI':\n",
        "\t\t\treturn 29\n",
        "\t\telif row_label=='V5-Normal':\n",
        "\t\t\treturn 30\t\n",
        "\t\telif row_label=='V5-PMI':\n",
        "\t\t\treturn 31\n",
        "\t\telif row_label=='V6-HB':\n",
        "\t\t\treturn 32\t\n",
        "\t\telif row_label=='V6-MI':\n",
        "\t\t\treturn 33\t\n",
        "\t\telif row_label=='V6-Normal':\n",
        "\t\t\treturn 34\t\n",
        "\t\telif row_label=='V6-PMI':\n",
        "\t\t\treturn 35\t\n",
        "\t\telif row_label=='aVF-HB':\n",
        "\t\t\treturn 36\t\n",
        "\t\telif row_label=='aVF-MI':\n",
        "\t\t\treturn 37\n",
        "\t\telif row_label=='aVF-Normal':\n",
        "\t\t\treturn 38\n",
        "\t\telif row_label=='aVF-PMI':\n",
        "\t\t\treturn 39\t\n",
        "\t\telif row_label=='aVL-HB':\n",
        "\t\t\treturn 40\n",
        "\t\telif row_label=='aVL-MI':\n",
        "\t\t\treturn 41\t\n",
        "\t\telif row_label=='aVL-Normal':\n",
        "\t\t\treturn 42\t\n",
        "\t\telif row_label=='aVL-PMI':\n",
        "\t\t\treturn 43\t\n",
        "\t\telif row_label=='aVR-HB':\n",
        "\t\t\treturn 44\t\n",
        "\t\telif row_label=='aVR-MI':\n",
        "\t\t\treturn 45\t\n",
        "\t\telif row_label=='aVR-Normal':\n",
        "\t\t\treturn 46\n",
        "\t\telif row_label=='aVR-PMI':\n",
        "\t\t\treturn 47\t\n",
        "\t\telif row_label=='V4-Normal':\n",
        "\t\t\treturn 48\n",
        "\t\telse:\n",
        "\t\t\treturn 0\n",
        "\n",
        "def split(df, group):\n",
        "  data = namedtuple('data', ['filename', 'object'])\n",
        "  gb = df.groupby(group)\n",
        "  return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]\n",
        "\n",
        "def create_tf_example(group, path):\n",
        "\twith tf.io.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n",
        "\t  encoded_jpg = fid.read()\n",
        "\tencoded_jpg_io = io.BytesIO(encoded_jpg)\n",
        "\timage = Image.open(encoded_jpg_io)\n",
        "\twidth, height = image.size\n",
        "\tfilename = group.filename.encode('utf8')\n",
        "\timage_format = b'jpg'\n",
        "\txmins = []\n",
        "\txmaxs = []\n",
        "\tymins = []\n",
        "\tymaxs = []\n",
        "\tclasses_text = []\n",
        "\tclasses = []\n",
        "\n",
        "\tfor index, row in group.object.iterrows():\n",
        "\t\txmins.append(row['xmin'] / width)\n",
        "\t\txmaxs.append(row['xmax'] / width)\n",
        "\t\tymins.append(row['ymin'] / height)\n",
        "\t\tymaxs.append(row['ymax'] / height)\n",
        "\t\tclasses_text.append(row['class'].encode('utf8'))\n",
        "\t\tclasses.append(class_text_to_int(row['class']))\n",
        "\n",
        "\ttf_example = tf.train.Example(features=tf.train.Features(feature={\n",
        "\t\t'image/height': dataset_util.int64_feature(height),\n",
        "\t\t'image/width': dataset_util.int64_feature(width),\n",
        "\t\t'image/filename': dataset_util.bytes_feature(filename),\n",
        "\t\t'image/source_id': dataset_util.bytes_feature(filename),\n",
        "\t\t'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
        "\t\t'image/format': dataset_util.bytes_feature(image_format),\n",
        "\t\t'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
        "\t\t'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
        "\t\t'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
        "\t\t'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
        "\t\t'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
        "\t\t'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
        "\t\t}))\n",
        "\treturn tf_example\n",
        "#creates tfrecord for both csv's\n",
        "for csv in ['train_labels', 'test_labels']:\n",
        "  writer = tf.io.TFRecordWriter(data_base_url + csv + '.record')\n",
        "  path = os.path.join(image_dir)\n",
        "  examples = pd.read_csv(data_base_url + csv + '.csv')\n",
        "  grouped = split(examples, 'filename')\n",
        "  for group in grouped:\n",
        "    tf_example = create_tf_example(group, path)\n",
        "    writer.write(tf_example.SerializeToString())\n",
        "    \n",
        "  writer.close()\n",
        "  output_path = os.path.join(os.getcwd(), data_base_url + csv + '.record')\n",
        "  print('Successfully created the TFRecords: {}'.format(data_base_url +csv + '.record'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_LUKhxMJKKI"
      },
      "source": [
        "# Some models to train on\n",
        "MODELS_CONFIG = {\n",
        "    'ssd_mobilenet_v2': {\n",
        "        'model_name': 'ssd_mobilenet_v2_coco_2018_03_29',\n",
        "    },\n",
        "    'faster_rcnn_inception_v2': {\n",
        "        'model_name': 'faster_rcnn_inception_v2_coco_2018_01_28',\n",
        "    },\n",
        "}\n",
        "\n",
        "# Select a model from `MODELS_CONFIG`.\n",
        "# I chose ssd_mobilenet_v2 for this project, you could choose any\n",
        "selected_model = 'ssd_mobilenet_v2'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOWAqa8HJbJe"
      },
      "source": [
        "#the distination folder where the model will be saved\n",
        "#change this if you have a different working dir\n",
        "DEST_DIR = '/drive/MyDrive/ECG_Project/data/models/research/pretrained_model'\n",
        "\n",
        "# Name of the object detection model to use.\n",
        "MODEL = MODELS_CONFIG[selected_model]['model_name']\n",
        "\n",
        "#selecting the model\n",
        "MODEL_FILE = MODEL + '.tar.gz'\n",
        "\n",
        "#creating the downlaod link for the model selected\n",
        "DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
        "\n",
        "#checks if the model has already been downloaded, download it otherwise\n",
        "if not (os.path.exists(MODEL_FILE)):\n",
        "    urllib.request.urlretrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n",
        "\n",
        "#unzipping the model and extracting its content\n",
        "tar = tarfile.open(MODEL_FILE)\n",
        "tar.extractall()\n",
        "tar.close()\n",
        "\n",
        "# creating an output file to save the model while training\n",
        "os.remove(MODEL_FILE)\n",
        "if (os.path.exists(DEST_DIR)):\n",
        "    shutil.rmtree(DEST_DIR)\n",
        "os.rename(MODEL, DEST_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKEYaLhdJunn"
      },
      "source": [
        "%%writefile object_detection/samples/configs/ssd_mobilenet_v2_coco.config\n",
        "\n",
        "model {\n",
        "  ssd {\n",
        "    num_classes: 48\n",
        "    box_coder {\n",
        "      faster_rcnn_box_coder {\n",
        "        y_scale: 10.0\n",
        "        x_scale: 10.0\n",
        "        height_scale: 5.0\n",
        "        width_scale: 5.0\n",
        "      }\n",
        "    }\n",
        "    matcher {\n",
        "      argmax_matcher {\n",
        "        matched_threshold: 0.5\n",
        "        unmatched_threshold: 0.5\n",
        "        ignore_thresholds: false\n",
        "        negatives_lower_than_unmatched: true\n",
        "        force_match_for_each_row: true\n",
        "      }\n",
        "    }\n",
        "    similarity_calculator {\n",
        "      iou_similarity {\n",
        "      }\n",
        "    }\n",
        "    anchor_generator {\n",
        "      ssd_anchor_generator {\n",
        "        num_layers: 6\n",
        "        min_scale: 0.2\n",
        "        max_scale: 0.95\n",
        "        aspect_ratios: 1.0\n",
        "        aspect_ratios: 2.0\n",
        "        aspect_ratios: 0.5\n",
        "        aspect_ratios: 3.0\n",
        "        aspect_ratios: 0.3333\n",
        "      }\n",
        "    }\n",
        "    image_resizer {\n",
        "      fixed_shape_resizer {\n",
        "        height: 300\n",
        "        width: 300\n",
        "      }\n",
        "    }\n",
        "    box_predictor {\n",
        "      convolutional_box_predictor {\n",
        "        min_depth: 0\n",
        "        max_depth: 0\n",
        "        num_layers_before_predictor: 0\n",
        "        use_dropout: false\n",
        "        dropout_keep_probability: 0.8\n",
        "        kernel_size: 1\n",
        "        box_code_size: 4\n",
        "        apply_sigmoid_to_scores: false\n",
        "        conv_hyperparams {\n",
        "          activation: RELU_6,\n",
        "          regularizer {\n",
        "            l2_regularizer {\n",
        "              weight: 0.00004\n",
        "            }\n",
        "          }\n",
        "          initializer {\n",
        "            truncated_normal_initializer {\n",
        "              stddev: 0.03\n",
        "              mean: 0.0\n",
        "            }\n",
        "          }\n",
        "          batch_norm {\n",
        "            train: true,\n",
        "            scale: true,\n",
        "            center: true,\n",
        "            decay: 0.9997,\n",
        "            epsilon: 0.001,\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    feature_extractor {\n",
        "      type: 'ssd_mobilenet_v2'\n",
        "      min_depth: 16\n",
        "      depth_multiplier: 1.0\n",
        "      conv_hyperparams {\n",
        "        activation: RELU_6,\n",
        "        regularizer {\n",
        "          l2_regularizer {\n",
        "            weight: 0.00004\n",
        "          }\n",
        "        }\n",
        "        initializer {\n",
        "          truncated_normal_initializer {\n",
        "            stddev: 0.03\n",
        "            mean: 0.0\n",
        "          }\n",
        "        }\n",
        "        batch_norm {\n",
        "          train: true,\n",
        "          scale: true,\n",
        "          center: true,\n",
        "          decay: 0.9997,\n",
        "          epsilon: 0.001,\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    loss {\n",
        "      classification_loss {\n",
        "        weighted_sigmoid {\n",
        "        }\n",
        "      }\n",
        "      localization_loss {\n",
        "        weighted_smooth_l1 {\n",
        "        }\n",
        "      }\n",
        "      hard_example_miner {\n",
        "        num_hard_examples: 3000\n",
        "        iou_threshold: 0.99\n",
        "        loss_type: CLASSIFICATION\n",
        "        max_negatives_per_positive: 3\n",
        "        min_negatives_per_image: 3\n",
        "      }\n",
        "      classification_weight: 1.0\n",
        "      localization_weight: 1.0\n",
        "    }\n",
        "    normalize_loss_by_num_matches: true\n",
        "    post_processing {\n",
        "      batch_non_max_suppression {\n",
        "        score_threshold: 1e-8\n",
        "        iou_threshold: 0.6\n",
        "        max_detections_per_class: 100\n",
        "        max_total_detections: 100\n",
        "      }\n",
        "      score_converter: SIGMOID\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "train_config: {\n",
        "  batch_size: 24\n",
        "  optimizer {\n",
        "    rms_prop_optimizer: {\n",
        "      learning_rate: {\n",
        "        exponential_decay_learning_rate {\n",
        "          initial_learning_rate: 0.004\n",
        "          decay_steps: 800720\n",
        "          decay_factor: 0.95\n",
        "        }\n",
        "      }\n",
        "      momentum_optimizer_value: 0.9\n",
        "      decay: 0.9\n",
        "      epsilon: 1.0\n",
        "    }\n",
        "  }\n",
        "  fine_tune_checkpoint:\"/drive/MyDrive/ECG_Project/data/models/research/pretrained_model/model.ckpt\"\n",
        "  fine_tune_checkpoint_type:  \"detection\"\n",
        "  # Note: The below line limits the training process to 200K steps, which we\n",
        "  # empirically found to be sufficient enough to train the pets dataset. This\n",
        "  # effectively bypasses the learning rate schedule (the learning rate will\n",
        "  # never decay). Remove the below line to train indefinitely.\n",
        "  num_steps: 200000\n",
        "  data_augmentation_options {\n",
        "    random_horizontal_flip {\n",
        "    }\n",
        "  }\n",
        "  data_augmentation_options {\n",
        "    ssd_random_crop {\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "train_input_reader: {\n",
        "  tf_record_input_reader {\n",
        "    input_path:\"/drive/MyDrive/ECG_Project/data/train_labels.record\"\n",
        "  }\n",
        "  label_map_path:\"/drive/MyDrive/ECG_Project/data/label_map.pbtxt\"\n",
        "}\n",
        "\n",
        "eval_config: {\n",
        "  num_examples: 8000\n",
        "  # Note: The below line limits the evaluation process to 10 evaluations.\n",
        "  # Remove the below line to evaluate indefinitely.\n",
        "  max_evals: 10\n",
        "}\n",
        "\n",
        "eval_input_reader: {\n",
        "  tf_record_input_reader {\n",
        "    input_path:\"/drive/MyDrive/ECG_Project/data/test_labels.record\"\n",
        "  }\n",
        "  label_map_path:\"/drive/MyDrive/ECG_Project/data/label_map.pbtxt\"\n",
        "  shuffle: false\n",
        "  num_readers: 1\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Y65KuzjJzCc"
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip -o ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQYcdan_JKRu"
      },
      "source": [
        "pip install lvis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPNJ_0jTJKb6"
      },
      "source": [
        "#the logs that are created while training \n",
        "LOG_DIR = \"training/\"\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "#The link to tensorboard.\n",
        "#works after the training starts.\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}